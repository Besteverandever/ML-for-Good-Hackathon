{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = \"C:\\\\Users\\\\user\\\\SadafPythonCode\\\\MLHackathon\\\\ML-for-Good-Hackathon\\\\Data\\\\\"\n",
    "\n",
    "df = pd.read_csv(root + 'CrisisLogger\\\\crisislogger.csv')\n",
    "\n",
    "#duplicated ids are joined in one transcription e.g. 436 and 441 are duplicate ids\n",
    "df = df.groupby(['upload_id'])['transcriptions'].apply(' '.join).reset_index()\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Autoencoder:\n",
    "    \"\"\"\n",
    "    Autoencoder for learning latent space representation\n",
    "    architecture simplified for only one hidden layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.activation = activation\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.autoencoder = None\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.his = None\n",
    "\n",
    "    def _compile(self, input_dim):\n",
    "        \"\"\"\n",
    "        compile the computational graph\n",
    "        \"\"\"\n",
    "        input_vec = Input(shape=(input_dim,))\n",
    "        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n",
    "        decoded = Dense(input_dim, activation=self.activation)(encoded)\n",
    "        self.autoencoder = Model(input_vec, decoded)\n",
    "        self.encoder = Model(input_vec, encoded)\n",
    "        encoded_input = Input(shape=(self.latent_dim,))\n",
    "        decoder_layer = self.autoencoder.layers[-1]\n",
    "        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n",
    "        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n",
    "\n",
    "    def fit(self, X):\n",
    "        if not self.autoencoder:\n",
    "            self._compile(X.shape[1])\n",
    "        X_train, X_test = train_test_split(X)\n",
    "        self.his = self.autoencoder.fit(X_train, X_train,\n",
    "                                        epochs=200,\n",
    "                                        batch_size=128,\n",
    "                                        shuffle=True,\n",
    "                                        validation_data=(X_test, X_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from language_detector import detect_language\n",
    "\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "if sym_spell.word_count:\n",
    "    pass\n",
    "else:\n",
    "    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "\n",
    "###################################\n",
    "#### sentence level preprocess ####\n",
    "###################################\n",
    "\n",
    "# lowercase + base filter\n",
    "# some basic normalization\n",
    "def f_base(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: processed string: see comments in the source code for more info\n",
    "    \"\"\"\n",
    "    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n",
    "    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n",
    "    # normalization 2: lower case\n",
    "    s = s.lower()\n",
    "    # normalization 3: \"&gt\", \"&lt\"\n",
    "    s = re.sub(r'&gt|&lt', ' ', s)\n",
    "    # normalization 4: letter repetition (if more than 2)\n",
    "    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n",
    "    # normalization 5: non-word repetition (if more than 1)\n",
    "    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n",
    "    # normalization 6: string * as delimiter\n",
    "    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n",
    "    # normalization 7: stuff in parenthesis, assumed to be less informal\n",
    "    s = re.sub(r'\\(.*?\\)', '. ', s)\n",
    "    # normalization 8: xxx[?!]. -- > xxx.\n",
    "    s = re.sub(r'\\W+?\\.', '.', s)\n",
    "    # normalization 9: [.?!] --> [.?!] xxx\n",
    "    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n",
    "    # normalization 10: ' ing ', noise text\n",
    "    s = re.sub(r' ing ', ' ', s)\n",
    "    # normalization 11: noise text\n",
    "    s = re.sub(r'product received for free[.| ]', ' ', s)\n",
    "    # normalization 12: phrase repetition\n",
    "    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "# language detection\n",
    "def f_lan(s):\n",
    "    \"\"\"\n",
    "    :param s: string to be processed\n",
    "    :return: boolean (s is English)\n",
    "    \"\"\"\n",
    "\n",
    "    # some reviews are actually english but biased toward french\n",
    "    return detect_language(s) in {'English', 'French','Spanish','Chinese'}\n",
    "\n",
    "\n",
    "###############################\n",
    "#### word level preprocess ####\n",
    "###############################\n",
    "\n",
    "# filtering out punctuations and numbers\n",
    "def f_punct(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with punct and number filter out\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word.isalpha()]\n",
    "\n",
    "\n",
    "# selecting nouns\n",
    "def f_noun(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with only nouns selected\n",
    "    \"\"\"\n",
    "    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n",
    "\n",
    "\n",
    "# typo correction\n",
    "def f_typo(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n",
    "    \"\"\"\n",
    "    w_list_fixed = []\n",
    "    for word in w_list:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n",
    "        if suggestions:\n",
    "            w_list_fixed.append(suggestions[0].term)\n",
    "        else:\n",
    "            pass\n",
    "            # do word segmentation, deprecated for inefficiency\n",
    "            # w_seg = sym_spell.word_segmentation(phrase=word)\n",
    "            # w_list_fixed.extend(w_seg.corrected_string.split())\n",
    "    return w_list_fixed\n",
    "\n",
    "\n",
    "# stemming if doing word-wise\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def f_stem(w_list):\n",
    "    \"\"\"\n",
    "    :param w_list: word list to be processed\n",
    "    :return: w_list with stemming\n",
    "    \"\"\"\n",
    "    return [p_stemmer.stem(word) for word in w_list]\n",
    "\n",
    "\n",
    "# filtering out stop words\n",
    "# create English stop words list\n",
    "\n",
    "stop_words = (list(\n",
    "    set(get_stop_words('en'))\n",
    "\n",
    "))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f_stopw(w_list):\n",
    "    \"\"\"\n",
    "    filtering out stop words\n",
    "    \"\"\"\n",
    "    return [word for word in w_list if word not in stop_words]\n",
    "\n",
    "\n",
    "def preprocess_sent(rw):\n",
    "    \"\"\"\n",
    "    Get sentence level preprocessed data from raw review texts\n",
    "    :param rw: review to be processed\n",
    "    :return: sentence level pre-processed review\n",
    "    \"\"\"\n",
    "    s = f_base(rw)\n",
    "    if not f_lan(s):\n",
    "        return None\n",
    "    return s\n",
    "\n",
    "\n",
    "def preprocess_word(s):\n",
    "    \"\"\"\n",
    "    Get word level preprocessed data from preprocessed sentences\n",
    "    including: remove punctuation, select noun, fix typo, stem, stop_words\n",
    "    :param s: sentence to be processed\n",
    "    :return: word level pre-processed review\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    w_list = word_tokenize(s)\n",
    "    w_list = f_punct(w_list)\n",
    "    w_list = f_noun(w_list)\n",
    "    w_list = f_typo(w_list)\n",
    "    #w_list = f_stem(w_list)\n",
    "    w_list = f_stopw(w_list)\n",
    "\n",
    "    return w_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(docs, samp_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess the data\n",
    "    \"\"\"\n",
    "    if not samp_size:\n",
    "        samp_size = 100\n",
    "\n",
    "    print('Preprocessing raw texts ...')\n",
    "    n_docs = len(docs)\n",
    "    sentences = []  # sentence level preprocessed\n",
    "    token_lists = [] # word level preprocessed\n",
    "    idx_in = []  # index of sample selected    \n",
    "    samp = np.random.choice(n_docs, samp_size) #random array of numbers for docs len\n",
    "    \n",
    "    for i, idx in enumerate(samp):\n",
    "        sentence = preprocess_sent(docs[idx])\n",
    "        token_list = preprocess_word(sentence)\n",
    "        if token_list:\n",
    "            idx_in.append(idx)\n",
    "            sentences.append(sentence)\n",
    "            token_lists.append(token_list)\n",
    "        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n",
    "    print('Preprocessing raw texts. Done!')\n",
    "    return sentences, token_lists, idx_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model object\n",
    "class Topic_Model:\n",
    "    def __init__(self, k=10, method='TFIDF'):\n",
    "        \"\"\"\n",
    "        :param k: number of topics\n",
    "        :param method: method chosen for the topic model\n",
    "        \"\"\"\n",
    "        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n",
    "            raise Exception('Invalid method!')\n",
    "        self.k = k\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        #         self.stopwords = None\n",
    "        self.cluster_model = None\n",
    "        self.ldamodel = None\n",
    "        self.vec = {}\n",
    "        self.gamma = 15  # parameter for reletive importance of lda\n",
    "        self.method = method\n",
    "        self.AE = None\n",
    "        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "    def vectorize(self, sentences, token_lists, method=None):\n",
    "        \"\"\"\n",
    "        Get vecotr representations from selected methods\n",
    "        \"\"\"\n",
    "        print(\"Topic_model.vectorize(): method= \", method)\n",
    "        # Default method\n",
    "        if method is None:\n",
    "            method = self.method\n",
    "\n",
    "        # turn tokenized documents into a id <-> term dictionary\n",
    "        self.dictionary = corpora.Dictionary(token_lists)\n",
    "        # convert tokenized documents into a document-term matrix\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        if method == 'TFIDF':\n",
    "            print('Getting vector representations for TF-IDF ...')\n",
    "            tfidf = TfidfVectorizer()\n",
    "            vec = tfidf.fit_transform(sentences)\n",
    "            print('Getting vector representations for TF-IDF. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'LDA':\n",
    "            print('Getting vector representations for LDA ...')\n",
    "            if not self.ldamodel:\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
    "                                                                passes=20)\n",
    "\n",
    "            def get_vec_lda(model, corpus, k):\n",
    "                \"\"\"\n",
    "                Get the LDA vector representation (probabilistic topic assignments for all documents)\n",
    "                :return: vec_lda with dimension: (n_doc * n_topic)\n",
    "                \"\"\"\n",
    "                n_doc = len(corpus)\n",
    "                vec_lda = np.zeros((n_doc, k))\n",
    "                for i in range(n_doc):\n",
    "                    # get the distribution for the i-th document in corpus\n",
    "                    for topic, prob in model.get_document_topics(corpus[i]):\n",
    "                        vec_lda[i, topic] = prob\n",
    "\n",
    "                return vec_lda\n",
    "\n",
    "            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n",
    "            print('Getting vector representations for LDA. Done!')\n",
    "            return vec\n",
    "\n",
    "        elif method == 'BERT':\n",
    "\n",
    "            print('Getting vector representations for BERT ...')\n",
    "            from sentence_transformers import SentenceTransformer\n",
    "            model = SentenceTransformer('bert-base-nli-max-tokens')\n",
    "            vec = np.array(model.encode(sentences, show_progress_bar=True))\n",
    "            print('Getting vector representations for BERT. Done!')\n",
    "            return vec\n",
    "\n",
    "             \n",
    "        elif method == 'LDA_BERT':\n",
    "        #else:\n",
    "            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n",
    "            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n",
    "            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n",
    "            self.vec['LDA_BERT_FULL'] = vec_ldabert\n",
    "            if not self.AE:\n",
    "                self.AE = Autoencoder()\n",
    "                print('Fitting Autoencoder ...')\n",
    "                self.AE.fit(vec_ldabert)\n",
    "                print('Fitting Autoencoder Done!')\n",
    "            vec = self.AE.encoder.predict(vec_ldabert)\n",
    "            return vec\n",
    "\n",
    "    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n",
    "        \"\"\"\n",
    "        Fit the topic model for selected method given the preprocessed data\n",
    "        :docs: list of documents, each doc is preprocessed as tokens\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Default method\n",
    "        if method is None:\n",
    "            method = self.method\n",
    "        # Default clustering method\n",
    "        if m_clustering is None:\n",
    "            m_clustering = KMeans\n",
    "\n",
    "        # turn tokenized documents into a id <-> term dictionary\n",
    "        if not self.dictionary:\n",
    "            self.dictionary = corpora.Dictionary(token_lists)\n",
    "            # convert tokenized documents into a document-term matrix\n",
    "            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "\n",
    "        ####################################################\n",
    "        #### Getting ldamodel or vector representations ####\n",
    "        ####################################################\n",
    "\n",
    "        if method == 'LDA':\n",
    "            if not self.ldamodel:\n",
    "                print('Fitting LDA ...')\n",
    "                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n",
    "                                                                passes=20)\n",
    "                print('Fitting LDA Done!')\n",
    "        else:\n",
    "            print('Clustering embeddings ...')\n",
    "            self.cluster_model = m_clustering(self.k)\n",
    "            self.vec[method] = self.vectorize(sentences, token_lists, method)\n",
    "            self.cluster_model.fit(self.vec[method])\n",
    "            print('Clustering embeddings. Done!')\n",
    "\n",
    "    def predict(self, sentences, token_lists, out_of_sample=None):\n",
    "        \"\"\"\n",
    "        Predict topics for new_documents\n",
    "        \"\"\"\n",
    "        # Default as False\n",
    "        out_of_sample = out_of_sample is not None\n",
    "\n",
    "        if out_of_sample:\n",
    "            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n",
    "            if self.method != 'LDA':\n",
    "                vec = self.vectorize(sentences, token_lists)                \n",
    "        else:\n",
    "            corpus = self.corpus\n",
    "            vec = self.vec.get(self.method, None)\n",
    "\n",
    "        if self.method == \"LDA\":\n",
    "            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n",
    "                                                     key=lambda x: x[1], reverse=True)[0][0],\n",
    "                                    corpus)))\n",
    "        else:\n",
    "            lbs = self.cluster_model.predict(vec)\n",
    "        return lbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def get_topic_words(token_lists, labels, k=None):\n",
    "    \"\"\"\n",
    "    get top words within each topic from clustering results\n",
    "    \"\"\"\n",
    "    if k is None:\n",
    "        k = len(np.unique(labels))\n",
    "    topics = ['' for _ in range(k)]\n",
    "    for i, c in enumerate(token_lists):\n",
    "        topics[labels[i]] += (' ' + ' '.join(c))\n",
    "    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n",
    "    # get sorted word counts\n",
    "    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n",
    "    # get topics\n",
    "    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n",
    "    print(\"get_topic_words: \", topics)\n",
    "    \n",
    "    return topics\n",
    "\n",
    "def get_coherence(model, token_lists, measure='c_v'):\n",
    "    \"\"\"\n",
    "    Get model coherence from gensim.models.coherencemodel\n",
    "    :param model: Topic_Model object\n",
    "    :param token_lists: token lists of docs\n",
    "    :param topics: topics as top words\n",
    "    :param measure: coherence metrics\n",
    "    :return: coherence score\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
    "                            coherence=measure)\n",
    "    else:\n",
    "        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n",
    "        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n",
    "                            coherence=measure)\n",
    "    return cm.get_coherence()\n",
    "\n",
    "def get_silhouette(model):\n",
    "    \"\"\"\n",
    "    Get silhouette score from model\n",
    "    :param model: Topic_Model object\n",
    "    :return: silhouette score\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    lbs = model.cluster_model.labels_\n",
    "    vec = model.vec[model.method]\n",
    "    return silhouette_score(vec, lbs)\n",
    "\n",
    "def plot_proj(embedding, lbs):\n",
    "    \"\"\"\n",
    "    Plot UMAP embeddings\n",
    "    :param embedding: UMAP (or other) embeddings\n",
    "    :param lbs: labels\n",
    "    \"\"\"\n",
    "    n = len(embedding)\n",
    "    counter = Counter(lbs)\n",
    "    for i in range(len(np.unique(lbs))):\n",
    "        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n",
    "                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n",
    "    plt.legend(loc = 'best')\n",
    "    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize(model):\n",
    "    \"\"\"\n",
    "    Visualize the result for the topic model by 2D embedding (UMAP)\n",
    "    :param model: Topic_Model object\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    reducer = umap.UMAP()\n",
    "    print('Calculating UMAP projection ...')\n",
    "    vec_umap = reducer.fit_transform(model.vec[model.method])\n",
    "    print('Calculating UMAP projection. Done!')    \n",
    "    plot_proj(vec_umap, model.cluster_model.labels_)\n",
    "    dr = '~\\\\images\\\\{}/{}'.format(model.method, model.id)\n",
    "    if not os.path.exists(dr):\n",
    "        os.makedirs(dr)\n",
    "    plt.savefig('~\\\\images')\n",
    "\n",
    "def get_wordcloud(model, token_lists, topic):\n",
    "    \"\"\"\n",
    "    Get word cloud of each topic from fitted model\n",
    "    :param model: Topic_Model object\n",
    "    :param sentences: preprocessed sentences from docs\n",
    "    \"\"\"\n",
    "    if model.method == 'LDA':\n",
    "        return\n",
    "    print('Getting wordcloud for topic {} ...'.format(topic))\n",
    "    lbs = model.cluster_model.labels_\n",
    "    print(\"get_wordcloud:\", lbs)\n",
    "    tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height=560,\n",
    "                          background_color='white', collocations=False,\n",
    "                          min_font_size=10).generate(tokens)\n",
    "\n",
    "    # plot the WordCloud image\n",
    "    plt.figure(figsize=(8, 5.6), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    dr = '/kaggle/working/{}/{}'.format(model.method, model.id)\n",
    "    if not os.path.exists(dr):\n",
    "        os.makedirs(dr)\n",
    "    plt.savefig('/kaggle/working' + '/Topic' + str(topic) + '_wordcloud')\n",
    "    print('Getting wordcloud for topic {}. Done!'.format(topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing raw texts ...\n",
      "Preprocessing raw texts. Done!\n"
     ]
    }
   ],
   "source": [
    "trns = df.transcriptions\n",
    "sentences, token_lists, idx_in = preprocess(trns, samp_size=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering embeddings ...\n",
      "Topic_model.vectorize(): method=  LDA_BERT\n",
      "Topic_model.vectorize(): method=  LDA\n",
      "Getting vector representations for LDA ...\n",
      "Getting vector representations for LDA. Done!\n",
      "Topic_model.vectorize(): method=  BERT\n",
      "Getting vector representations for BERT ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2195acdc86b747b6bb7ebc1e945ccab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting vector representations for BERT. Done!\n",
      "Fitting Autoencoder ...\n",
      "Fitting Autoencoder Done!\n",
      "Clustering embeddings. Done!\n",
      "get_topic_words:  [['time', 'school', 'home', 'son', 'lot', 'things', 'daughter', 'husband', 'people', 'family'], ['family', 'time', 'friends', 'day', 'kids', 'lot', 'things', 'everything', 'children', 'year'], ['time', 'care', 'home', 'health', 'times', 'days', 'fears', 'asthma', 'stress', 'doctor'], ['school', 'time', 'kids', 'home', 'children', 'work', 'year', 'day', 'lot', 'month'], ['time', 'people', 'things', 'children', 'home', 'week', 'virus', 'friends', 'pray', 'one']]\n",
      "Coherence: 0.5504470325293901\n",
      "Silhouette Score: 0.6818922\n",
      "Calculating UMAP projection ...\n"
     ]
    }
   ],
   "source": [
    "method = \"LDA_BERT\"\n",
    "ntopic = 5\n",
    "# Define the topic model object\n",
    "tm = Topic_Model(k = ntopic, method = method)\n",
    "# Fit the topic model by chosen method\n",
    "tm.fit(sentences, token_lists)\n",
    "print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n",
    "print('Silhouette Score:', get_silhouette(tm))\n",
    "visualize(tm)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mybert",
   "language": "python",
   "name": "mybert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Reading in and processing Word documents (Focus Group data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sourcing packages\n",
    "- The textract package is used to read in the .docx files.\n",
    "- The gensim package is used to fit prelimnary LDA models on the data and filter out words which are common to the majority of the identified topics.\n",
    "- The nltk package is used to get an initial list of stopwords and for word lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "import numpy as np\n",
    "import scipy\n",
    "import gensim\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim import corpora, models\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of FocusGroup class \n",
    "### Instantiation\n",
    "- By giving the name of the word file. The word file should have the same format as the focus group documents, that is, each paragraph should be preceeded by a line specifying the name (e.g. Parent 1) of the currently speaking person.\n",
    "### Attributes\n",
    "- raw_text: The raw text from the Word document.\n",
    "- parent_moderator_discussion: The part of raw_text which refers to the discussion between parents and moderators. The rationale for separating the parent_moderator_discussion and within_moderator_discussion attributes is that there was a case when there was a discussion only between the moderators after the discusion between parents and moderators.\n",
    "- text_including_parents: An np.array storing the discussion between the parents and moderators. Each element of the np.array contains a paragraph from the discussion.\n",
    "- talkers_including_parents: An np. array with an identical size as text_including_parents containing the respective talker's name (e.g. Parent 1).\n",
    "- within_moderator_discussion: The part of raw_text which refers to the discussion only moderators, if available. This part of the text was separated from the parent / moderator discussion part of the text by two blank lines.\n",
    "- text_only_moderators: An np.array storing the discussion only between the moderators, if available. Each element of the np.array contains a paragraph from the discussion.\n",
    "- talkers_only_moderators: An np. array with an identical size as text_only_moderators containing the respective talker's name (e.g. Moderator 1).\n",
    "- parent_list: List of unique parent participants.\n",
    "- moderator_list: List of unique moderator participants.\n",
    "### Methods\n",
    "- get_participant_text(participant): gets the list of paragraphs which belong to the given participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocusGroup:\n",
    "    def __init__(self, filename):\n",
    "        self.raw_text=str(textract.process('Data/FocusGroups/' + filename + \".docx\")).replace('b\\'', '').replace('\\'', '')\n",
    "        \n",
    "        self.parent_moderator_discussion=self.raw_text.split('\\\\n\\\\n\\\\n')[0].split('\\\\n\\\\n')\n",
    "        self.text_including_parents=np.array([parent_moderator_actual\n",
    "                                    for parent_moderator_actual in self.parent_moderator_discussion \n",
    "                                    if not (('Parent'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or \n",
    "                                        ('Moderator'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or \n",
    "                                        ('Administrator'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or\n",
    "                                        ('Speaker'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)))])\n",
    "        self.talkers_including_parents=np.array([parent_moderator_actual.replace(':', '') \n",
    "                                    for parent_moderator_actual in self.parent_moderator_discussion \n",
    "                                    if (('Parent'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or \n",
    "                                        ('Moderator'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or \n",
    "                                        ('Administrator'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or\n",
    "                                        ('Speaker'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)))])\n",
    "        \n",
    "        if len(self.raw_text.split('\\\\n\\\\n\\\\n'))>1:\n",
    "            self.within_moderator_discussion=self.raw_text.split('\\\\n\\\\n\\\\n')[1].split('\\\\n\\\\n')\n",
    "            self.text_only_moderators=np.array([parent_moderator_actual\n",
    "                                    for parent_moderator_actual in self.within_moderator_discussion \n",
    "                                    if not (('Parent'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or \n",
    "                                        ('Moderator'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or \n",
    "                                        ('Administrator'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or\n",
    "                                        ('Speaker'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)))])\n",
    "            self.talkers_only_moderators=np.array([parent_moderator_actual.replace(':', '') \n",
    "                                    for parent_moderator_actual in self.within_moderator_discussion \n",
    "                                    if (('Parent'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or \n",
    "                                        ('Moderator'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or \n",
    "                                        ('Administrator'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)) or\n",
    "                                        ('Speaker'==re.sub(r\" [0-9]:\",\"\",parent_moderator_actual)))])\n",
    "        \n",
    "        self.parent_list=[participant for participant in set(self.talkers_including_parents) if 'Parent' in participant]\n",
    "        self.moderator_list=[participant for participant in set(self.talkers_including_parents) if 'Moderator' in participant]\n",
    "        \n",
    "        \n",
    "    def get_participant_text(self, participant):\n",
    "        if 'Parent' in participant:\n",
    "            mask=[member==participant for member in self.talkers_including_parents]\n",
    "            return list(self.text_including_parents[mask])\n",
    "        elif 'Moderator' in participant:\n",
    "            mask=[member==participant for member in self.talkers_including_parents]\n",
    "            text_from_parent_discussion=self.text_including_parents[mask]\n",
    "            \n",
    "            if len(self.raw_text.split('\\\\n\\\\n\\\\n'))==1:\n",
    "                return list(text_from_parent_discussion)\n",
    "            else:\n",
    "                mask=[member==participant for member in self.talkers_only_moderators]\n",
    "                text_from_moderator_discussion=self.text_only_moderators[mask]\n",
    "                return list(text_from_parent_discussion) + list(text_from_moderator_discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to process text\n",
    "- The original list of stopwords was augmented by stopwords which are filler words (for example, okay) or are the consequences of the automated transcription (for example, inaudible), this extra list of stopwords was saved under the custom_stopwords list.\n",
    "- The WordNetLemmatizer() class of the nltk library was used for lemmatization.\n",
    "- The following data processing steps are performed by the text_processing_pipeline function: \n",
    "    - Making the string lowercase\n",
    "    - Removal of punctuation\n",
    "    - Tokenization\n",
    "    - Removal of text with less than min_token_count tokens\n",
    "    - Removing stopwords\n",
    "    - Lemmatization\n",
    "    - Removing stopwords (also after the lemmatization)\n",
    "- The output of the text processing pipeline is a list with the elements, the first element is the processed, tokenized text and the second element is the original text with the purpose to help with the intepretation of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list=stopwords.words('english')\n",
    "custom_stopwords=['go','parent','say','0','yeah','would','okay','start','also','well','u','thank','inaudible','crosstalk','able','hear','actually','hi','oh','definitely','part','anything','sure','anyone','yes','thanks','everything','end','everybody','tand','administrator','whatever','sound','ti','moderator','though','mute','speak','silence','finish','bye','audio']\n",
    "stopwords_list=stopwords_list+custom_stopwords\n",
    "remove_stopwords_function=lambda tokenized_text, stopwords: [word for word in tokenized_text if word not in stopwords]\n",
    "lemmatizer_instance=WordNetLemmatizer()\n",
    "pos_tags_lemmatize_mapping_dict={'N': 'n', 'V': 'v', 'J': 'a', 'R': 'r'}\n",
    "\n",
    "def pos_mapping_function(pos_tag, dictionary=pos_tags_lemmatize_mapping_dict):\n",
    "    if pos_tag[0] in ['N', 'V', 'J', 'R']:\n",
    "        return dictionary[pos_tag[0]]\n",
    "    else:\n",
    "        return 'n'\n",
    "    \n",
    "def lemmatizer_function(text, dictionary=pos_tags_lemmatize_mapping_dict, pos_mapping_function=pos_mapping_function,\n",
    "                       lemmatizer=lemmatizer_instance):\n",
    "    pos_tags_for_lemmatize=[(word, pos_mapping_function(pos_tag)) for word, pos_tag in nltk.pos_tag(text)]\n",
    "    pos_tags_lemmatized=[lemmatizer_instance.lemmatize(word, pos=pos_tag) for word, pos_tag in pos_tags_for_lemmatize]\n",
    "    return pos_tags_lemmatized\n",
    "\n",
    "def text_processing_pipeline(text_list,additional_stopwords, min_token_count=1, stopwords_list=stopwords_list, \n",
    "                             lemmatizer_function=lemmatizer_function, dictionary=pos_tags_lemmatize_mapping_dict,\n",
    "                             pos_mapping_function=pos_mapping_function, lemmatizer=lemmatizer_instance):\n",
    "    stopwords_list=stopwords_list+additional_stopwords\n",
    "    lowercase_text_list=[text.lower() for text in text_list] #Making text lowercase\n",
    "    lowercase_text_list=[re.sub(r\"[^a-zA-Z0-9]\", \" \", text) for text in lowercase_text_list] #Removal of punctuation\n",
    "    lowercase_text_list=[text.split() for text in lowercase_text_list] #Tokenization\n",
    "    filtering_original_text=[text_list[i] for i in range (len(lowercase_text_list)) if len(lowercase_text_list[i])>min_token_count]\n",
    "    lowercase_text_list=[text for text in lowercase_text_list if len(text)>min_token_count] #Keeping text with an at least a pre-defined token count\n",
    "    lowercase_text_list=[remove_stopwords_function(text, stopwords_list) for text in lowercase_text_list] #Removing stopwords\n",
    "    lowercase_text_list=[lemmatizer_function(text) for text in lowercase_text_list] #Lemmatization\n",
    "    lowercase_text_list=[remove_stopwords_function(text, stopwords_list) for text in lowercase_text_list] #Removing stopwords\n",
    "    return lowercase_text_list, filtering_original_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the word data\n",
    "- Loop over the forteen Word documents with the text processing function and save the result in a list with 15 elements.\n",
    "- The below code cell contains four lists of additional stopwords for Gaming group / Low PIU group / Media group and Social group, respectively, this set of additional stopwords was generated by Module 2 by iteratively running the gensim LDA algorithm and excluding the words which were included in at least 3 of the 5 topics. The purpose of this data processing step is to avoid having the same set of words in all topics.\n",
    "- The min_token_count of the text_processing_pipeline function was set to 60, so only paragraphs with at least 60 tokens were kept in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=['Gaming_Group1', 'Gaming_Group2', 'Gaming_Group3', 'Gaming_Group4',\n",
    "           'LowPIU_Group1', 'LowPIU_Group2', 'LowPIU_Group3',\n",
    "           'Media_Group1', 'Media_Group2', 'Media_Group3', 'Media_Group4',\n",
    "           'Social_Group1', 'Social_Group2', 'Social_Group3', 'Social_Group4']\n",
    "additional_stopword_counts=list(dict(Counter([re.sub('[0-9]', '', file,) for file in file_list])).values())\n",
    "Gaming_group_stopwords=['like', 'get', 'school', 'hour', 'day', 'even', 'think', 'thing', 'way', 'know', 'year', 'week', 'really', 'one',\n",
    "                       'kid', 'game', 'use', 'time', 'want', 'play', 'much', 'back']\n",
    "Low_PIU_group_stopwords=['school', 'like', 'time', 'get', 'think', 'kid', 'really',\n",
    "                        'thing', '00', 'technology', 'year', 'child', 'back', 'lot',\n",
    "                        'even', 'know', 'want', 'old', 'one']\n",
    "Media_group_stopwords=['like', 'thing', 'get', 'really', 'kid', 'time', 'want',\n",
    "                      'school', 'think', 'know', 'one', 'use',\n",
    "                      'year', 'much', 'back', 'work', 'person', 'pandemic',\n",
    "                      'see', 'lot', 'good', 'little', 'day', 'old']\n",
    "Social_group_stopwords=['like', 'get', 'think', 'know', 'thing', 'time', 'school',\n",
    "                       'really', 'child', 'see', 'want',\n",
    "                       'kid', 'one', 'lot', 'even']\n",
    "additional_stopwords_list=[Gaming_group_stopwords, Low_PIU_group_stopwords, Media_group_stopwords, Social_group_stopwords]\n",
    "additional_stopwords_list=[[stopword_list]*count for count, stopword_list in zip(additional_stopword_counts, additional_stopwords_list)]\n",
    "additional_stopwords_list=[stopword for additional_stopword in additional_stopwords_list for stopword in additional_stopword]\n",
    "all_focusgroup_text=[FocusGroup(focus_group_file) for focus_group_file in file_list]\n",
    "all_focusgroup_processed_text=[text_processing_pipeline(focus_group.text_including_parents,additional_stopword_list, min_token_count=60) for focus_group, additional_stopword_list in zip(all_focusgroup_text, additional_stopwords_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_focusgroup_processed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
